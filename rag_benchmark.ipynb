{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9970386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HOME\"] = \"/mnt/nas/shuvranshu\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/nas/shuvranshu/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/mnt/nas/shuvranshu/huggingface_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/mnt/nas/shuvranshu/huggingface_cache\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/mnt/nas/shuvranshu/huggingface_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/mnt/nas/shuvranshu/huggingface_cache\"\n",
    "\n",
    "os.makedirs(\"/mnt/nas/shuvranshu/huggingface_cache\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de71297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nas/shuvranshu/.conda/envs/ragenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/nas/shuvranshu/.conda/envs/ragenv/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "#hf token \n",
    "load_dotenv()  \n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb850ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]\n",
      "Device set to use cuda:2\n",
      "/tmp/ipykernel_2390915/864760465.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"/mnt/nas/shuvranshu/huggingface_cache/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\",model_kwargs={\"device\": \"cuda:2\"})\n",
      "No sentence-transformers model found with name /mnt/nas/shuvranshu/huggingface_cache/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf. Creating a new one with mean pooling.\n",
      "/tmp/ipykernel_2390915/864760465.py:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    # model_id=\"/mnt/nas/shuvranshu/huggingface_cache/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\", \n",
    "    model_id=\"meta-llama/Llama-3.1-8B\",\n",
    "    # model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    "    device=2\n",
    ")\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/mnt/nas/shuvranshu/huggingface_cache/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\",model_kwargs={\"device\": \"cuda:2\"})\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 40})\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Use the following context to answer the question.\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "\n",
    "def predict_fn(question: str) -> str:\n",
    "    return qa.run(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87dc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "dataset=load_dataset(\"squad_v2\",split=\"validation[:100]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b5149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When were the Normans in Normandy?\n",
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
      "{'text': ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries'], 'answer_start': [94, 87, 94, 94]}\n"
     ]
    }
   ],
   "source": [
    "# print(len(dataset[5][\"answers\"][\"text\"]))\n",
    "print(dataset[1][\"question\"])\n",
    "print(dataset[1][\"context\"])\n",
    "print(dataset[1][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad579af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:0 completed\n",
      "question:1 completed\n",
      "question:2 completed\n",
      "question:3 completed\n",
      "question:4 completed\n",
      "question:5 completed\n",
      "question:6 completed\n",
      "question:7 completed\n",
      "question:8 completed\n",
      "question:9 completed\n",
      "question:10 completed\n",
      "question:11 completed\n",
      "question:12 completed\n",
      "question:13 completed\n",
      "question:14 completed\n",
      "question:15 completed\n",
      "question:16 completed\n",
      "question:17 completed\n",
      "question:18 completed\n",
      "question:19 completed\n",
      "question:20 completed\n",
      "question:21 completed\n",
      "question:22 completed\n",
      "question:23 completed\n",
      "question:24 completed\n",
      "question:25 completed\n",
      "question:26 completed\n",
      "question:27 completed\n",
      "question:28 completed\n",
      "question:29 completed\n",
      "question:30 completed\n",
      "question:31 completed\n",
      "question:32 completed\n",
      "question:33 completed\n",
      "question:34 completed\n",
      "question:35 completed\n",
      "question:36 completed\n",
      "question:37 completed\n",
      "question:38 completed\n",
      "question:39 completed\n",
      "question:40 completed\n",
      "question:41 completed\n",
      "question:42 completed\n",
      "question:43 completed\n",
      "question:44 completed\n",
      "question:45 completed\n",
      "question:46 completed\n",
      "question:47 completed\n",
      "question:48 completed\n",
      "question:49 completed\n",
      "question:50 completed\n",
      "question:51 completed\n",
      "question:52 completed\n",
      "question:53 completed\n",
      "question:54 completed\n",
      "question:55 completed\n",
      "question:56 completed\n",
      "question:57 completed\n",
      "question:58 completed\n",
      "question:59 completed\n",
      "question:60 completed\n",
      "question:61 completed\n",
      "question:62 completed\n",
      "question:63 completed\n",
      "question:64 completed\n",
      "question:65 completed\n",
      "question:66 completed\n",
      "question:67 completed\n",
      "question:68 completed\n",
      "question:69 completed\n",
      "question:70 completed\n",
      "question:71 completed\n",
      "question:72 completed\n",
      "question:73 completed\n",
      "question:74 completed\n",
      "question:75 completed\n",
      "question:76 completed\n",
      "question:77 completed\n",
      "question:78 completed\n",
      "question:79 completed\n",
      "question:80 completed\n",
      "question:81 completed\n",
      "question:82 completed\n",
      "question:83 completed\n",
      "question:84 completed\n",
      "question:85 completed\n",
      "question:86 completed\n",
      "question:87 completed\n",
      "question:88 completed\n",
      "question:89 completed\n",
      "question:90 completed\n",
      "question:91 completed\n",
      "question:92 completed\n",
      "question:93 completed\n",
      "question:94 completed\n",
      "question:95 completed\n",
      "question:96 completed\n",
      "question:97 completed\n",
      "question:98 completed\n",
      "question:99 completed\n"
     ]
    }
   ],
   "source": [
    "questions=[]\n",
    "ground_truths=[]\n",
    "q=0\n",
    "for row in dataset:\n",
    "    questions.append(row[\"question\"])\n",
    "    vectorstore.add_texts(row[\"context\"])\n",
    "    if(len(row[\"answers\"][\"text\"])):\n",
    "        ground_truths.append(row[\"answers\"][\"text\"][0])\n",
    "    else:\n",
    "        ground_truths.append(\"\")\n",
    "    \n",
    "    \n",
    "    print(f\"question:{q} completed\")\n",
    "    q+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c0345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2390915/175219143.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs=retriever.get_relevant_documents(question)\n",
      "/tmp/ipykernel_2390915/175219143.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result=qa.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:0, no of contexts retrieved:40\n",
      "question:1, no of contexts retrieved:40\n",
      "question:2, no of contexts retrieved:40\n",
      "question:3, no of contexts retrieved:40\n",
      "question:4, no of contexts retrieved:40\n",
      "question:5, no of contexts retrieved:40\n",
      "question:6, no of contexts retrieved:40\n",
      "question:7, no of contexts retrieved:40\n",
      "question:8, no of contexts retrieved:40\n",
      "question:9, no of contexts retrieved:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:10, no of contexts retrieved:40\n",
      "question:11, no of contexts retrieved:40\n",
      "question:12, no of contexts retrieved:40\n",
      "question:13, no of contexts retrieved:40\n",
      "question:14, no of contexts retrieved:40\n",
      "question:15, no of contexts retrieved:40\n",
      "question:16, no of contexts retrieved:40\n",
      "question:17, no of contexts retrieved:40\n",
      "question:18, no of contexts retrieved:40\n",
      "question:19, no of contexts retrieved:40\n",
      "question:20, no of contexts retrieved:40\n",
      "question:21, no of contexts retrieved:40\n",
      "question:22, no of contexts retrieved:40\n",
      "question:23, no of contexts retrieved:40\n",
      "question:24, no of contexts retrieved:40\n",
      "question:25, no of contexts retrieved:40\n",
      "question:26, no of contexts retrieved:40\n",
      "question:27, no of contexts retrieved:40\n",
      "question:28, no of contexts retrieved:40\n",
      "question:29, no of contexts retrieved:40\n",
      "question:30, no of contexts retrieved:40\n",
      "question:31, no of contexts retrieved:40\n",
      "question:32, no of contexts retrieved:40\n",
      "question:33, no of contexts retrieved:40\n",
      "question:34, no of contexts retrieved:40\n",
      "question:35, no of contexts retrieved:40\n",
      "question:36, no of contexts retrieved:40\n",
      "question:37, no of contexts retrieved:40\n",
      "question:38, no of contexts retrieved:40\n",
      "question:39, no of contexts retrieved:40\n",
      "question:40, no of contexts retrieved:40\n",
      "question:41, no of contexts retrieved:40\n",
      "question:42, no of contexts retrieved:40\n",
      "question:43, no of contexts retrieved:40\n",
      "question:44, no of contexts retrieved:40\n",
      "question:45, no of contexts retrieved:40\n",
      "question:46, no of contexts retrieved:40\n",
      "question:47, no of contexts retrieved:40\n",
      "question:48, no of contexts retrieved:40\n",
      "question:49, no of contexts retrieved:40\n",
      "question:50, no of contexts retrieved:40\n",
      "question:51, no of contexts retrieved:40\n",
      "question:52, no of contexts retrieved:40\n",
      "question:53, no of contexts retrieved:40\n",
      "question:54, no of contexts retrieved:40\n",
      "question:55, no of contexts retrieved:40\n",
      "question:56, no of contexts retrieved:40\n",
      "question:57, no of contexts retrieved:40\n",
      "question:58, no of contexts retrieved:40\n",
      "question:59, no of contexts retrieved:40\n",
      "question:60, no of contexts retrieved:40\n",
      "question:61, no of contexts retrieved:40\n",
      "question:62, no of contexts retrieved:40\n",
      "question:63, no of contexts retrieved:40\n",
      "question:64, no of contexts retrieved:40\n",
      "question:65, no of contexts retrieved:40\n",
      "question:66, no of contexts retrieved:40\n",
      "question:67, no of contexts retrieved:40\n",
      "question:68, no of contexts retrieved:40\n",
      "question:69, no of contexts retrieved:40\n",
      "question:70, no of contexts retrieved:40\n",
      "question:71, no of contexts retrieved:40\n",
      "question:72, no of contexts retrieved:40\n",
      "question:73, no of contexts retrieved:40\n",
      "question:74, no of contexts retrieved:40\n",
      "question:75, no of contexts retrieved:40\n",
      "question:76, no of contexts retrieved:40\n",
      "question:77, no of contexts retrieved:40\n",
      "question:78, no of contexts retrieved:40\n",
      "question:79, no of contexts retrieved:40\n",
      "question:80, no of contexts retrieved:40\n",
      "question:81, no of contexts retrieved:40\n",
      "question:82, no of contexts retrieved:40\n",
      "question:83, no of contexts retrieved:40\n",
      "question:84, no of contexts retrieved:40\n",
      "question:85, no of contexts retrieved:40\n",
      "question:86, no of contexts retrieved:40\n",
      "question:87, no of contexts retrieved:40\n",
      "question:88, no of contexts retrieved:40\n",
      "question:89, no of contexts retrieved:40\n",
      "question:90, no of contexts retrieved:40\n",
      "question:91, no of contexts retrieved:40\n",
      "question:92, no of contexts retrieved:40\n",
      "question:93, no of contexts retrieved:40\n",
      "question:94, no of contexts retrieved:40\n",
      "question:95, no of contexts retrieved:40\n",
      "question:96, no of contexts retrieved:40\n",
      "question:97, no of contexts retrieved:40\n",
      "question:98, no of contexts retrieved:40\n",
      "question:99, no of contexts retrieved:40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag_answers=[]\n",
    "retrieved_contexts=[]\n",
    "q=0\n",
    "for question in questions:\n",
    "    docs=retriever.get_relevant_documents(question)\n",
    "    context = [d.page_content for d in docs] \n",
    "    print(f\"question:{q}, no of contexts retrieved:{len(context)}\")\n",
    "    q+=1\n",
    "    retrieved_contexts.append(context)\n",
    "    result=qa.run(question)\n",
    "    rag_answers.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a202af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"contexts\": retrieved_contexts,\n",
    "    \"reference\": ground_truths,\n",
    "    \"response\": rag_answers,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_2353995/127511802.py:30: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  ragas_llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "# #ragas evaluation\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from datasets import Dataset\n",
    "\n",
    "\n",
    "# model_name=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/mnt/nas/shuvranshu/huggingface_cache\")\n",
    "# ragas_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=\"/mnt/nas/shuvranshu/huggingface_cache\",\n",
    "#     # device_map=\"auto\",            \n",
    "#     offload_folder=\"/mnt/nas/shuvranshu/offload\",  \n",
    "#     dtype=torch.float16,\n",
    "#     device_map={\"\": 0} #force gpu usage\n",
    "# )\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     task=\"text-generation\",\n",
    "#     model=ragas_model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=128,\n",
    "#     temperature=0.1,\n",
    "#     device_map={\"\": 2}\n",
    "# )\n",
    "\n",
    "# ragas_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "# ragas_dataset = Dataset.from_pandas(data)\n",
    "# ragas_dataset_small = ragas_dataset.select(range(2))\n",
    "\n",
    "\n",
    "# # from ragas import evaluate\n",
    "# # from ragas.metrics import answer_correctness, faithfulness, context_precision\n",
    "# # from ragas.evaluation import RunConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = RunConfig(\n",
    "#     timeout=7200,      # 2 hour per job\n",
    "#     max_workers=1      # run serially\n",
    "# )\n",
    "\n",
    "# result = evaluate(\n",
    "#     ragas_dataset_small,\n",
    "#     metrics=[answer_correctness, faithfulness, context_precision],\n",
    "#     llm=ragas_llm,\n",
    "#     embeddings=embeddings,\n",
    "#     # return_executor=True\n",
    "# )\n",
    "\n",
    "# # for job in result.jobs:\n",
    "# #     job_input, job_output, job_error, job_traceback = job\n",
    "# #     print(\"Input:\", job_input)\n",
    "# #     print(\"Raw model output:\", job_output)\n",
    "# #     # The parsing error is now in job_error\n",
    "# #     print(\"Parsing error (if any):\", job_error)\n",
    "# #     print(\"Full Traceback:\", job_traceback)\n",
    "# #     print(\"----\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c362d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average hallucination score for dataset: 0.44\n"
     ]
    }
   ],
   "source": [
    "#hallucination score\n",
    "import re\n",
    "\n",
    "def simple_hallucination_score(answer: str, context: str) -> float:\n",
    "    context_words = set(re.findall(r\"\\w+\", str(context).lower()))\n",
    "    answer_words = re.findall(r\"\\w+\", str(answer).lower())\n",
    "\n",
    "    if not answer_words:\n",
    "        return 0.0\n",
    "\n",
    "    hallucinated_words = [w for w in answer_words if w not in context_words]\n",
    "    score = len(hallucinated_words) / len(answer_words)\n",
    "    return score\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# def semantic_hallucination_score(answer: str, context: str, threshold: float = 0.7) -> float:\n",
    "#     \"\"\"\n",
    "#     Computes a hallucination score using semantic similarity.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - answer: generated answer string\n",
    "#     - context: reference context string\n",
    "#     - threshold: similarity threshold to consider a word/phrase supported\n",
    "    \n",
    "#     Returns:\n",
    "#     - score: fraction of unsupported content in the answer\n",
    "#     \"\"\"\n",
    "#     if not answer.strip():\n",
    "#         return 0.0\n",
    "\n",
    "#     # Split the answer into phrases or sentences\n",
    "#     answer_sentences = [s.strip() for s in answer.split('.') if s.strip()]\n",
    "#     context_embedding = model.encode(context, convert_to_tensor=True)\n",
    "    \n",
    "#     hallucinated_count = 0\n",
    "#     for sent in answer_sentences:\n",
    "#         sent_embedding = model.encode(sent, convert_to_tensor=True)\n",
    "#         similarity = util.cos_sim(sent_embedding, context_embedding).item()\n",
    "#         if similarity < threshold:\n",
    "#             hallucinated_count += 1\n",
    "    \n",
    "#     score = hallucinated_count / len(answer_sentences)\n",
    "#     return score\n",
    "\n",
    "hallucination_scores=[]\n",
    "ragas_dataset = Dataset.from_pandas(data)\n",
    "for entry in ragas_dataset:\n",
    "    generated_answer = entry[\"response\"]\n",
    "    context = \" \".join(entry[\"contexts\"])  # assuming contexts is a list of strings\n",
    "    # context=entry[\"contexts\"]\n",
    "    score = simple_hallucination_score(generated_answer, context)\n",
    "    hallucination_scores.append(score)\n",
    "\n",
    "avg_score = sum(hallucination_scores) / len(hallucination_scores)\n",
    "print(f\"\\nAverage hallucination score for dataset: {avg_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
